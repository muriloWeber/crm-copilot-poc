# src/front_end/app.py

import sys
import os
import streamlit as st
import time
import logging
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, AIMessage # <--- NOVA IMPORTAÃ‡ÃƒO AQUI
from langgraph.graph import StateGraph, END

logging.basicConfig(level=logging.INFO) 

# Adiciona a raiz do projeto ao sys.path para que o Python encontre o pacote 'src'
# independentemente de como o Streamlit Ã© executado.
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root) # Insere no inÃ­cio para dar prioridade

# --- Importar as definiÃ§Ãµes do agente que criamos ---
# Estes imports agora funcionarÃ£o porque a raiz do projeto estÃ¡ no sys.path
from src.core.copilot_agent import AgentState, retrieve_context_node, generate_response_node, format_citation_node

# Carrega as variÃ¡veis do .env no inÃ­cio do script
load_dotenv()

# --- ConfiguraÃ§Ã£o do Agente LangGraph ---
def create_agent_workflow():
    workflow = StateGraph(AgentState)

    workflow.add_node("retrieve_context", retrieve_context_node)
    workflow.add_node("generate_response", generate_response_node)
    workflow.add_node("format_citation", format_citation_node)

    workflow.set_entry_point("retrieve_context")
    workflow.add_edge("retrieve_context", "generate_response")
    workflow.add_edge("generate_response", "format_citation")
    workflow.add_edge("format_citation", END)

    return workflow.compile()

# Instanciar o workflow uma vez
# Isso tambÃ©m inicializa o modelo de embeddings e o ChromaDB
# Certifique-se que o build_knowledge_base jÃ¡ foi executado!
try:
    copilot_agent = create_agent_workflow()
    st.session_state.llm_initialized = True
except Exception as e:
    st.error(f"Erro ao inicializar o Copilot: {e}. Certifique-se de que a base de conhecimento foi construÃ­da e as variÃ¡veis de ambiente estÃ£o corretas.")
    st.session_state.llm_initialized = False


# --- TCRM Copilot Streamlit UI ---
st.set_page_config(page_title="TCRM Copilot", page_icon="ï¿½ï¿½")

st.title("ðŸ¤– TCRM Copilot")
st.markdown("Seu assistente de IA para projetos TOTVS CRM.")

# Inicializar histÃ³rico de chat na sessÃ£o do Streamlit
if "messages" not in st.session_state:
    st.session_state.messages = []
# Ensure llm_initialized is set even if not initially found, to prevent KeyError
if "llm_initialized" not in st.session_state:
    st.session_state.llm_initialized = False # Default to False if not set by try/except


# Display de mensagens anteriores
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Entrada de chat
if prompt := st.chat_input("Pergunte ao Copilot sobre seu projeto..."):
    if not st.session_state.llm_initialized:
        st.warning("O Copilot nÃ£o foi inicializado corretamente. Verifique os logs.")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # VariÃ¡vel para acumular a simulaÃ§Ã£o de digitaÃ§Ã£o
        accumulated_text_for_display = ""
        # VariÃ¡vel para guardar a resposta FINAL do agente
        agent_final_response_content = ""

        try:
            # --- CONVERSÃƒO DAS MENSAGENS E ESTADO INICIAL COMPLETO (PONTO CRÃTICO CORRIGIDO!) ---
            lc_messages = []
            for msg in st.session_state.messages:
                if msg["role"] == "user":
                    lc_messages.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    lc_messages.append(AIMessage(content=msg["content"]))

            # Prepare initial state for the agent, ensuring all AgentState fields are initialized
            initial_agent_state = AgentState(
                question=prompt,
                context=[],         # SerÃ¡ populado por retrieve_context_node
                source_docs=[],     # SerÃ¡ populado por retrieve_context_node
                answer="",          # SerÃ¡ populado por generate_response_node
                messages=lc_messages, # Mensagens convertidas para o formato LangChain
                filters={}          # Inicializado como dicionÃ¡rio vazio
            )

            # Invocar o agente LangGraph
            response = copilot_agent.invoke(initial_agent_state) # <--- Passando o estado inicial completo
            
            # A resposta final formatada jÃ¡ deve estar em response['answer']
            agent_final_response_content = response.get("answer", "NÃ£o consegui gerar uma resposta para isso. Tente refazer a pergunta ou fornecer mais contexto.")

            # Simular digitaÃ§Ã£o usando a variÃ¡vel temporÃ¡ria
            for chunk in agent_final_response_content.split(" "):
                accumulated_text_for_display += chunk + " "
                time.sleep(0.05)
                message_placeholder.markdown(accumulated_text_for_display + "â–Œ")
            
            # Exiba a resposta COMPLETA e FINAL (sem o cursor piscando)
            message_placeholder.markdown(agent_final_response_content)

        except Exception as e:
            st.error(f"Ocorreu um erro ao processar sua pergunta: {e}")
            agent_final_response_content = "Ops! Parece que algo deu errado. Por favor, tente novamente."
            message_placeholder.markdown(agent_final_response_content) # Exibe a mensagem de erro

    # Apenas UMA VEZ, adicione a resposta final (limpa e completa) ao histÃ³rico da sessÃ£o
    st.session_state.messages.append({"role": "assistant", "content": agent_final_response_content})